"""Simple, dependency-free heuristic detector.

This detector uses lightweight features (length, vocabulary variety,
repetition, and common AI phrases) to approximate whether text is
AI-generated, human-written, or suspicious.
"""
import sys
import os
import re
from collections import Counter
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'schemas')))
from models import DetectionRequest, DetectionResult
from detector import Detector, DetectorRegistry


AI_HINTS = [
    "as an ai language model",
    "i'm unable to",
    "cannot browse the internet",
    "large language model",
    "generated by ai",
]

FORMAL_PHRASES = [
    "in conclusion",
    "in summary",
    "furthermore",
    "moreover",
    "additionally",
    "overall",
    "in addition",
    "therefore",
]


class SimpleDetector(Detector):
    """A lightweight heuristic-based detector with richer signals."""

    def __init__(self, config: dict):
        super().__init__(config)
        # Length above which we start treating text as "long" for suspicion
        self.threshold_len = int(config.get("threshold_len", 600))

    @property
    def model_name(self) -> str:
        return "simple-heuristic"

    async def detect(self, request: DetectionRequest) -> DetectionResult:
        text = request.text or ""
        lower = text.lower()

        # Tokenization & basic stats
        words = re.findall(r"\b\w+\b", lower)
        word_count = len(words)
        char_count = len(text)

        if word_count:
            avg_word_len = sum(len(w) for w in words) / word_count
            unique_ratio = len(set(words)) / float(word_count)
            freq = Counter(words)
            max_freq = max(freq.values())
            repetition_score = max_freq / float(word_count)
        else:
            avg_word_len = 0.0
            unique_ratio = 0.0
            repetition_score = 0.0

        # Sentence-level stats (very rough)
        sentences = [s.strip() for s in re.split(r"[.!?]+", text) if s.strip()]
        sentence_count = max(1, len(sentences))
        avg_sentence_len = word_count / float(sentence_count)

        # Punctuation density
        punct_count = sum(1 for ch in text if ch in ",;:()[]{}")
        punctuation_ratio = (punct_count / float(char_count)) if char_count else 0.0

        # Heuristic signals
        ai_hints_present = [h for h in AI_HINTS if h in lower]
        has_ai_hint = bool(ai_hints_present)
        formal_hits = [p for p in FORMAL_PHRASES if p in lower]
        has_formal = bool(formal_hits)
        is_long = char_count >= self.threshold_len

        # Short text: don't overthink it
        if word_count < 5 and not has_ai_hint:
            label = "human-written"
            confidence = 0.6
        else:
            # Build an AI-likelihood score in [0, 1]
            ai_score = 0.0

            if has_ai_hint:
                ai_score += 0.5
            if has_formal:
                ai_score += 0.15
            if avg_word_len >= 5.5:
                ai_score += 0.1
            if avg_sentence_len >= 20:
                ai_score += 0.1
            if repetition_score <= 0.12 and unique_ratio >= 0.6:
                ai_score += 0.1
            if is_long:
                ai_score += 0.1
            if char_count < 120 and not has_ai_hint:
                ai_score -= 0.15

            ai_score = max(0.0, min(1.0, ai_score))

            if ai_score >= 0.7:
                # Examples that tend to hit this branch:
                # - Long, formal essays with many connectors ("in conclusion", "furthermore")
                # - Text explicitly mentioning AI ("as an AI language model", "generated by AI")
                label = "AI-generated"
                confidence = 0.8 + 0.15 * (ai_score - 0.7)  # 0.8 - 0.95
            elif ai_score >= 0.4:
                # Examples that tend to hit this branch:
                # - Medium-length technical text with mixed informal phrases and structure
                # - Content that is long but lacks strong AI hints or is partially edited by a human
                label = "suspicious"
                confidence = 0.6 + 0.2 * (ai_score - 0.4) / 0.3  # ~0.6 - 0.8
            else:
                # Examples that tend to hit this branch:
                # - Short conversational messages or notes with simple vocabulary
                # - Personal anecdotes or highly idiosyncratic writing with some repetition
                label = "human-written"
                confidence = 0.6 + 0.2 * (0.4 - ai_score) / 0.4  # ~0.6 - 0.8

        confidence = float(max(0.0, min(1.0, confidence)))

        return DetectionResult(
            label=label,
            confidence=confidence,
            model_name=self.model_name,
            timestamp=datetime.utcnow(),
            metadata={
                "detector": "simple",
                "signals": {
                    "text_length": char_count,
                    "word_count": word_count,
                    "avg_word_length": round(avg_word_len, 2),
                    "unique_word_ratio": round(unique_ratio, 3),
                    "repetition_score": round(repetition_score, 3),
                    "sentence_count": sentence_count,
                    "avg_sentence_length": round(avg_sentence_len, 2),
                    "punctuation_ratio": round(punctuation_ratio, 3),
                    "ai_score": round(ai_score if word_count >= 5 or has_ai_hint else 0.0, 3),
                },
                "ai_hints": ai_hints_present,
                "formal_phrases": formal_hits,
            },
        )


# Register this detector
DetectorRegistry.register("simple", SimpleDetector)
